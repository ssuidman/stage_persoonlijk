% Encoding: UTF-8

@Article{Anderson2014,
  author    = {Anderson, David J. and Perona, Pietro},
  journal   = {Neuron},
  title     = {Toward a Science of Computational Ethology},
  year      = {2014},
  issn      = {0896-6273},
  number    = {1},
  pages     = {18--31},
  volume    = {84},
  abstract  = {The new field of ?Computational Ethology? is made possible by advances in technology, mathematics, and engineering that allow scientists to automate the measurement and the analysis of animal behavior. We explore the opportunities and long-term directions of research in this area.
The new field of ?Computational Ethology? is made possible by advances in technology, mathematics, and engineering that allow scientists to automate the measurement and the analysis of animal behavior. We explore the opportunities and long-term directions of research in this area.},
  comment   = {doi: 10.1016/j.neuron.2014.09.005},
  doi       = {10.1016/j.neuron.2014.09.005},
  publisher = {Elsevier},
  url       = {http://dx.doi.org/10.1016/j.neuron.2014.09.005},
}


@Article{Datta2019,
  author    = {Datta, Sandeep Robert and Anderson, David J. and Branson, Kristin and Perona, Pietro and Leifer, Andrew},
  title     = {Computational Neuroethology: A Call to Action},
  journal   = {Neuron},
  year      = {2019},
  volume    = {104},
  number    = {1},
  month     = oct,
  pages     = {11--24},
  issn      = {0896-6273},
  doi       = {10.1016/j.neuron.2019.09.038},
  url       = {https://doi.org/10.1016/j.neuron.2019.09.038},
  abstract  = {The brain is worthy of study because it is in charge of behavior. A flurry of recent technical advances in measuring and quantifying naturalistic behaviors provide an important opportunity for advancing brain science. However, the problem of understanding unrestrained behavior in the context of neural recordings and manipulations remains unsolved, and developing approaches to addressing this challenge is critical. Here we discuss considerations in computational neuroethology?the science of quantifying naturalistic behaviors for understanding the brain?and propose strategies to evaluate progress. We point to open questions that require resolution and call upon the broader systems neuroscience community to further develop and leverage measures of naturalistic, unrestrained behavior, which will enable us to more effectively probe the richness and complexity of the brain.
The brain is worthy of study because it is in charge of behavior. A flurry of recent technical advances in measuring and quantifying naturalistic behaviors provide an important opportunity for advancing brain science. However, the problem of understanding unrestrained behavior in the context of neural recordings and manipulations remains unsolved, and developing approaches to addressing this challenge is critical. Here we discuss considerations in computational neuroethology?the science of quantifying naturalistic behaviors for understanding the brain?and propose strategies to evaluate progress. We point to open questions that require resolution and call upon the broader systems neuroscience community to further develop and leverage measures of naturalistic, unrestrained behavior, which will enable us to more effectively probe the richness and complexity of the brain.},
  comment   = {doi: 10.1016/j.neuron.2019.09.038},
  publisher = {Elsevier},
}


@Article{Hong2015,
  author          = {Hong, Weizhe and Kennedy, Ann and Burgos-Artizzu, Xavier P and Zelikowsky, Moriel and Navonne, Santiago G and Perona, Pietro and Anderson, David J},
  title           = {Automated measurement of mouse social behaviors using depth sensing, video tracking, and machine learning.},
  doi             = {10.1073/pnas.1515982112},
  issn            = {1091-6490},
  issue           = {38},
  pages           = {E5351--E5360},
  volume          = {112},
  abstract        = {A lack of automated, quantitative, and accurate assessment of social behaviors in mammalian animal models has limited progress toward understanding mechanisms underlying social interactions and their disorders such as autism. Here we present a new integrated hardware and software system that combines video tracking, depth sensing, and machine learning for automatic detection and quantification of social behaviors involving close and dynamic interactions between two mice of different coat colors in their home cage. We designed a hardware setup that integrates traditional video cameras with a depth camera, developed computer vision tools to extract the body "pose" of individual animals in a social context, and used a supervised learning algorithm to classify several well-described social behaviors. We validated the robustness of the automated classifiers in various experimental settings and used them to examine how genetic background, such as that of Black and Tan Brachyury (BTBR) mice (a previously reported autism model), influences social behavior. Our integrated approach allows for rapid, automated measurement of social behaviors across diverse experimental designs and also affords the ability to develop new, objective behavioral metrics.},
  citation-subset = {IM},
  completed       = {2016-01-01},
  country         = {United States},
  groups          = {[arne:]},
  issn-linking    = {0027-8424},
  journal         = {Proceedings of the National Academy of Sciences of the United States of America},
  keywords        = {Algorithms; Animals; Behavior, Animal; Computers; Female; Image Processing, Computer-Assisted; Machine Learning; Male; Mice; Mice, Inbred BALB C; Mice, Inbred C57BL; Models, Animal; Observer Variation; Pattern Recognition, Automated; Reproducibility of Results; Social Behavior; Software; Video Recording; behavioral tracking; depth sensing; machine vision; social behavior; supervised machine learning},
  month           = sep,
  nlm-id          = {7505876},
  owner           = {NLM},
  pii             = {1515982112},
  pmc             = {PMC4586844},
  pmid            = {26354123},
  pubmodel        = {Print-Electronic},
  pubstatus       = {ppublish},
  revised         = {2018-11-13},
  year            = {2015},
}


@Article{Chaumont2012,
  author          = {de Chaumont, Fabrice and Coura, Renata Dos-Santos and Serreau, Pierre and Cressant, Arnaud and Chabout, Jonathan and Granon, Sylvie and Olivo-Marin, Jean-Christophe},
  title           = {Computerized video analysis of social interactions in mice.},
  journal         = {Nature methods},
  year            = {2012},
  volume          = {9},
  issue           = {4},
  month           = mar,
  pages           = {410--417},
  issn            = {1548-7105},
  doi             = {10.1038/nmeth.1924},
  abstract        = {The study of social interactions in mice is used as a model for normal and pathological cognitive and emotional processes. But extracting comprehensive behavioral information from videos of interacting mice is still a challenge. We describe a computerized method and software, MiceProfiler, that uses geometrical primitives to model and track two mice without requiring any specific tagging. The program monitors a comprehensive repertoire of behavioral states and their temporal evolution, allowing the identification of key elements that trigger social contact. Using MiceProfiler we studied the role of neuronal nicotinic receptors in the establishment of social interactions and risk-prone postures. We found that the duration and type of social interactions with a conspecific evolves differently over time in mice lacking neuronal nicotinic receptors (Chrnb2-/-, here called β2(-/-)), compared to C57BL/6J mice, and identified a new type of coordinated posture, called back-to-back posture, that we rarely observed in β2(-/-) mice.},
  chemicals       = {Receptors, Nicotinic, nicotinic receptor beta2},
  citation-subset = {IM},
  completed       = {2012-06-25},
  country         = {United States},
  issn-linking    = {1548-7091},
  keywords        = {Animals; Automation; Choice Behavior; Image Processing, Computer-Assisted; Learning; Male; Mice; Mice, Inbred C57BL; Mice, Knockout; Posture, physiology; Receptors, Nicotinic, deficiency, metabolism; Reproducibility of Results; Social Behavior; Software; Time Factors; Video Recording; Visual Fields, physiology},
  nlm-id          = {101215604},
  owner           = {NLM},
  pii             = {nmeth.1924},
  pmid            = {22388289},
  pubmodel        = {Electronic},
  pubstatus       = {epublish},
  revised         = {2018-11-13},
}


@Article{Mathis2018,
  author       = {Mathis, Alexander and Mamidanna, Pranav and Cury, Kevin M and Abe, Taiga and Murthy, Venkatesh N and Mathis, Mackenzie Weygandt and Bethge, Matthias},
  title        = {DeepLabCut: markerless pose estimation of user-defined body parts with deep learning.},
  journal      = {Nature neuroscience},
  year         = {2018},
  volume       = {21},
  issue        = {9},
  month        = sep,
  pages        = {1281--1289},
  issn         = {1546-1726},
  doi          = {10.1038/s41593-018-0209-y},
  abstract     = {Quantifying behavior is crucial for many applications in neuroscience. Videography provides easy methods for the observation and recording of animal behavior in diverse settings, yet extracting particular aspects of a behavior for further analysis can be highly time consuming. In motor control studies, humans or other animals are often marked with reflective markers to assist with computer-based tracking, but markers are intrusive, and the number and location of the markers must be determined a priori. Here we present an efficient method for markerless pose estimation based on transfer learning with deep neural networks that achieves excellent results with minimal training data. We demonstrate the versatility of this framework by tracking various body parts in multiple species across a broad collection of behaviors. Remarkably, even when only a small number of frames are labeled (~200), the algorithm achieves excellent tracking performance on test frames that is comparable to human accuracy.},
  country      = {United States},
  issn-linking = {1097-6256},
  nlm-id       = {9809671},
  owner        = {NLM},
  pii          = {10.1038/s41593-018-0209-y},
  pmid         = {30127430},
  pubmodel     = {Print-Electronic},
  pubstatus    = {ppublish},
  revised      = {2018-08-31},
}

@Article{Meyer2020.02.20.957712,
  author       = {Meyer, Arne F. and O{\textquoteright}Keefe, John and Poort, Jasper},
  title        = {Two distinct types of eye-head coupling in freely moving mice},
  journal      = {bioRxiv},
  year         = {2020},
  abstract     = {Animals actively interact with their environment to gather sensory information. There is conflicting evidence about how mice use vision to sample their environment. During head restraint, mice make rapid eye movements strongly coupled between the eyes, similar to conjugate saccadic eye movements in humans. However, when mice are free to move their heads, eye movement patterns are more complex and often non-conjugate, with the eyes moving in opposite directions. Here, we combined eye tracking with head motion measurements in freely moving mice and found that both observations can be explained by the existence of two distinct types of coupling between eye and head movements. The first type comprised non-conjugate eye movements which systematically compensated for changes in head tilt to maintain approximately the same visual field relative to the horizontal ground plane. The second type of eye movements were conjugate and coupled to head yaw rotation to produce a {\textquotedblleft}saccade and fixate{\textquotedblright} gaze pattern. During head initiated saccades, the eyes moved together in the same direction as the head, but during subsequent fixation moved in the opposite direction to the head to compensate for head rotation. This {\textquotedblleft}saccade and fixate{\textquotedblright} pattern is similar to that seen in humans who use eye movements (with or without head movement) to rapidly shift gaze but in mice relies on combined eye and head movements. Indeed, the two types of eye movements very rarely occurred in the absence of head movements. Even in head-restrained mice, eye movements were invariably associated with attempted head motion. Both types of eye-head coupling were seen in freely moving mice during social interactions and a visually-guided object tracking task. Our results reveal that mice use a combination of head and eye movements to sample their environment and highlight the similarities and differences between eye movements in mice and humans.HighlightsTracking of eyes and head in freely moving mice reveals two types of eye-head couplingEye/head tilt coupling aligns gaze to horizontal planeRotational eye and head coupling produces a {\textquotedblleft}saccade and fixate{\textquotedblright} gaze pattern with head leading the eyeBoth types of eye-head coupling are maintained during visually-guided behaviorsEye movements in head-restrained mice are related to attempted head movements},
  doi          = {10.1101/2020.02.20.957712},
  elocation-id = {2020.02.20.957712},
  eprint       = {https://www.biorxiv.org/content/early/2020/02/20/2020.02.20.957712.full.pdf},
  publisher    = {Cold Spring Harbor Laboratory},
  url          = {https://www.biorxiv.org/content/early/2020/02/20/2020.02.20.957712},
}

@Comment{jabref-meta: databaseType:bibtex;}
